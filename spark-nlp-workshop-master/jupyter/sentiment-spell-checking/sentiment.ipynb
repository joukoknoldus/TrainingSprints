{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vivekn Sentiment analysis computes Vivek Nayamanan algorithm from a set of positive and negative corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load SparkSession if not already there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ner\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"2G\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"../../lib/sparknlp.jar\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"500m\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a spark dataset and put it in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the input data to be annotated\n",
    "data = spark.createDataFrame([\n",
    "    [\"Harry Potter was a great movie!\"],\n",
    "    [\"I totally recommend it\"],\n",
    "    [\"Awesome, just awesome!!\"],\n",
    "    [\"Very bad movie, don't even watch it\"]\n",
    "]).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We creat the document assemblerr, which will put target text column into Annotation form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the dataframe\n",
    "document_assembler = DocumentAssembler() \\\n",
    "            .setInputCol(\"text\")\n",
    "        \n",
    "### Transform input to appropriate schema\n",
    "#assembled = document_assembler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence detector will parse sub sentences in every line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sentence detector\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "#sentence_data = sentence_detector.transform(checked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer will match standard tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer\n",
    "tokenizer = Tokenizer() \\\n",
    "            .setInputCols([\"sentence\"]) \\\n",
    "            .setOutputCol(\"token\")\n",
    "#tokenized = tokenizer.transform(assembled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizer will clean out the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer() \\\n",
    "            .setInputCols([\"token\"]) \\\n",
    "            .setOutputCol(\"normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spell checker will correct normalized tokens, this trains with a dictionary of english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Spell Checker\n",
    "spell_checker = NorvigSweetingApproach() \\\n",
    "            .setInputCols([\"normal\"]) \\\n",
    "            .setOutputCol(\"spell\") \\\n",
    "            .setDictionary(\"../../data/spell/words.txt\") \\\n",
    "            .setCorpus(\"../../data/spell/coca2017.txt\")\n",
    "\n",
    "#checked = spell_checker.fit(tokenized).transform(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We creat the ViveknSentimentApproach and set resources to train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_detector = ViveknSentimentApproach() \\\n",
    "    .setInputCols([\"spell\", \"sentence\"]) \\\n",
    "    .setOutputCol(\"sentiment\") \\\n",
    "    .setPruneCorpus(0) \\\n",
    "    .setPositiveSource(\"../../data/vivekn/training_positive\") \\\n",
    "    .setNegativeSource(\"../../data/vivekn/training_negative\") \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finisher will utilize sentiment analysis output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"sentiment\"]) \\\n",
    "    .setIncludeKeys(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit and predict over data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    spell_checker,\n",
    "    sentiment_detector,\n",
    "    finisher\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "sentiment_model = pipeline.fit(data)\n",
    "end = time.time()\n",
    "print(\"Time elapsed pipeline process: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data = sentiment_model.transform(data)\n",
    "sentiment_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a sample back into the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in sentiment_data.take(5):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster way to compute tests, streaming or small amounts of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import LightPipeline\n",
    "\n",
    "light = LightPipeline(sentiment_model)\n",
    "light.annotate(\"So boring, I didn't like it at all\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
